{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1169e6b5",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this final project, we'll attempt to predict the type of physical activity (e.g., walking, climbing stairs) from tri-axial smartphone accelerometer data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ae154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import scipy.stats as stats\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start=time.process_time() #Run Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the datasets\n",
    "train_df = pd.read_csv('train_time_series.csv')\n",
    "train_labels_df = pd.read_csv('train_labels.csv')\n",
    "test_df = pd.read_csv('test_time_series.csv')\n",
    "test_labels_df = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34560250",
   "metadata": {},
   "source": [
    "#### Clean the data\n",
    "Need to include labels with the training data.  Since the train_labels_df only has every 10th datapoint, the labels in the concatenated df's will need to be filled out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the training data\n",
    "df1 = train_df.copy()\n",
    "df2 = train_labels_df.copy()\n",
    "df1 = df1.set_index('Unnamed: 0')\n",
    "df2 = df2.set_index('Unnamed: 0')\n",
    "train_df_with_labels = pd.concat([df1,df2['label']],axis=1)\n",
    "train_df_with_labels = train_df_with_labels.fillna(method='bfill')\n",
    "\n",
    "# train_df_with_labels.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the test data\n",
    "df1 = test_df.copy()\n",
    "df2 = test_labels_df.copy()\n",
    "df1 = df1.set_index('Unnamed: 0')\n",
    "df2 = df2.set_index('Unnamed: 0')\n",
    "test_df_with_labels = pd.concat([df1,df2['label']],axis=1)\n",
    "\n",
    "# test_df_with_labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9a897",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# quick glance at the training data\n",
    "plt.plot(train_df_with_labels['timestamp'],train_df_with_labels['x'], label='x')\n",
    "plt.plot(train_df_with_labels['timestamp'],train_df_with_labels['y'], label='y')\n",
    "plt.plot(train_df_with_labels['timestamp'],train_df_with_labels['z'], label='z')\n",
    "plt.plot(train_df_with_labels['timestamp'],train_df_with_labels['label'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1aef2",
   "metadata": {},
   "source": [
    "The x and z data seeem to be mostly centererd around 0 while the y accelerometer data is slightly offset. There is also  an uneven distribution of the differnt activities.  1 = standing, 2 = walking, 3 = stairs down, 4 = stairs up. I will filter the data by activity to examine it more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "standing = train_df_with_labels[train_df_with_labels.label == 1]\n",
    "walking = train_df_with_labels[train_df_with_labels.label == 2]\n",
    "downstairs = train_df_with_labels[train_df_with_labels.label == 3]\n",
    "upstairs = train_df_with_labels[train_df_with_labels.label == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1cb8c0",
   "metadata": {},
   "source": [
    "Examine the filtered dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977070bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set up multiindex df to display summary stats\n",
    "activities = ['standing','walking','downstairs','upstairs']\n",
    "features = ['x','y','z']\n",
    "index = pd.MultiIndex.from_product([activities, features])\n",
    "\n",
    "dfs = [standing, walking, downstairs, upstairs]\n",
    "features_summary_df = pd.DataFrame(index = ['count','mean','stdev', 'min','max'], columns=index)\n",
    "for i in range(len(dfs)):\n",
    "    for j in range(len(features)):\n",
    "        features_summary_df[activities[i],features[j]] = [dfs[i][features[j]].count(),\n",
    "                                                    dfs[i][features[j]].mean(), \n",
    "                                                  dfs[i][features[j]].std(), \n",
    "                                                  dfs[i][features[j]].min(), \n",
    "                                                  dfs[i][features[j]].max()] \n",
    "\n",
    "features_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539382c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 3d plots  to display the accellerometer data filtered by activity\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "def subplot(df,label,position):\n",
    "    ax = fig.add_subplot(position,projection='3d')\n",
    "    ax.scatter(df['x'],df['y'],df['z'],label=label)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "subplot(standing,'standing',221)\n",
    "subplot(walking,'walking',222)\n",
    "subplot(downstairs,'downstairs',223)\n",
    "subplot(upstairs,'upstairs',224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560cdba",
   "metadata": {},
   "source": [
    "At a glance, the standing data is visibly different to the other activities, but the other activities are not easily distinguished from each other.  The other obvious feature is a very small number of observations for standing.  Ideally I would want to balance the data so we are working with an equal number of observations for each activity, but I think that limiting the size of the dataset that much would hurt more than help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8367c",
   "metadata": {},
   "source": [
    "### Testing different models\n",
    "\n",
    "We covered several regression models during the course, but it is not clear which one might be best suited for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data I'm going to use for the different models\n",
    "\n",
    "X=np.array(train_df_with_labels[['x','y','z']])\n",
    "y=np.array(train_df_with_labels['label'])\n",
    "test=np.array(test_df_with_labels[['x','y','z']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5706c859",
   "metadata": {},
   "source": [
    "#### Logisitc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9122f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.5) # split the data\n",
    "\n",
    "logmodel=LogisticRegression()\n",
    "logmodel.fit(X_train,y_train) # fit the model\n",
    "print(f'Score: {logmodel.score(X_test,y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.5)\n",
    "\n",
    "forestmodel=RandomForestClassifier()\n",
    "forestmodel.fit(X_train,y_train)\n",
    "print(f'Score: {forestmodel.score(X_test,y_test)}')\n",
    "print(classification_report(y_test,forestmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e2013",
   "metadata": {},
   "source": [
    "#### Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b44d80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.5)\n",
    "\n",
    "knnmodel=KNeighborsClassifier(n_neighbors=5)\n",
    "knnmodel.fit(X_train,y_train)\n",
    "print(f'{knnmodel.score(X_test,y_test)}')\n",
    "print(classification_report(y_test,knnmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264c597",
   "metadata": {},
   "source": [
    "#### CNN\n",
    "convolutional neural networks were not covered in the course, but I found an example of a cnn being used to predict outcomes from similar data, so I will compare that to the outcome from the regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while the x,y and z data all seem to be of a similar scale, there are some discrepancies in the mean and variance\n",
    "# so I will standardize the data before running the CNN\n",
    "\n",
    "X = train_df_with_labels[['x', 'y', 'z']]\n",
    "y = train_df_with_labels['label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "scaled_X = pd.DataFrame(data = X, columns = ['x', 'y', 'z'])\n",
    "scaled_X['label'] = y.values\n",
    "\n",
    "scaled_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca746f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide data into a series of individual timeframes\n",
    "Fs = 10 # 10 data points per second\n",
    "frame_size = Fs*4 # 4 second frames\n",
    "step_size = Fs*2 # 2 second steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(df, frame_size, step_size):\n",
    "\n",
    "    N_FEATURES = 3\n",
    "\n",
    "    frames = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - frame_size, step_size):\n",
    "        x = train_df_with_labels['x'].values[i: i + frame_size]\n",
    "        y = train_df_with_labels['y'].values[i: i + frame_size]\n",
    "        z = train_df_with_labels['z'].values[i: i + frame_size]\n",
    "\n",
    "        \n",
    "        # Retrieve the most often used label in this segment\n",
    "        label = stats.mode(train_df_with_labels['label'].iloc[i: i + frame_size], keepdims=True)[0][0]\n",
    "        frames.append([x, y, z])\n",
    "        labels.append(label)\n",
    "\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    frames = np.asarray(frames).reshape(-1, frame_size, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return frames, labels\n",
    "\n",
    "X,y = get_frames(scaled_X, frame_size, step_size)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8879ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(X_train[0].shape, X_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reshape the data to fit the model\n",
    "X_train = X_train.reshape(148, 40, 3, 1)\n",
    "X_test = X_test.reshape(38, 40, 3, 1)\n",
    "print(X_train[0].shape, X_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45057a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (2, 2), activation = 'relu', input_shape = X_train[0].shape))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(32, (2, 2), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6db862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and fit the model\n",
    "model.compile(optimizer=Adam(learning_rate = 0.001), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs = 20, validation_data= (X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_learningCurve(history, epochs):\n",
    "  # Plot training & validation accuracy values\n",
    "  epoch_range = range(1, epochs+1)\n",
    "  plt.plot(epoch_range, history.history['accuracy'])\n",
    "  plt.plot(epoch_range, history.history['val_accuracy'])\n",
    "  plt.title('Model accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(epoch_range, history.history['loss'])\n",
    "  plt.plot(epoch_range, history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa955b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learningCurve(history, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d025e88",
   "metadata": {},
   "source": [
    "The CNN model does not appear to perform any better with this data than the other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25dc266",
   "metadata": {},
   "source": [
    "### Training and fitting the model\n",
    "Overall, the Knn model gave the best results so I will choose that model to predict the activities of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the knn model\n",
    "X_train=np.array(train_df_with_labels[['x','y','z']])\n",
    "y_train=np.array(train_df_with_labels['label'])\n",
    "# test data\n",
    "test=np.array(test_df_with_labels[['x','y','z']])\n",
    "\n",
    "final_model=KNeighborsClassifier(n_neighbors=5)\n",
    "final_model.fit(X_train,y_train)\n",
    "\n",
    "final_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef14ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predict labels for test data based on trained model\n",
    "label_predictions=final_model.predict(test)\n",
    "test_df_with_labels['label']=label_predictions\n",
    "#save as csv for submission\n",
    "test_df_with_labels.to_csv('test labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e5fde7",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "I've tried various approaches but I didn't see any large differences in the outcome between the approaches.  The same CNN model I used here produced much higher accuracies with a different data set (Tesnsor flow analysis of accelerometer data - https://kgptalkie.com/human-activity-recognition-using-accelerometer-data/).  As best as I can tell, the main limiting factor here is the small size of the data set.  Normally, it would be better to use a balanced data set (similar numbers of observations for each activity), but the very small number of observations for standing in this dataset means that balancing the dataset would have made it too small to be reliable. It is possible that tweaking the parameters in the CNN model might have improved the outcome, but I need to spend more time to understand that model better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759d4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
